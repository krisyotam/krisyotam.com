# ==============================================================================
# DOCUMENT: simulating-moral-judgment-llms.mdx
# TYPE:     PAPERS
#
# RATIONALE:
#   This document uses human-readable YAML front matter as a durable metadata
#   layer. In the event of database loss or corruption, content and metadata
#   can be reconstructed directly from source files.
#
# REQUIREMENTS:
#   - YAML front matter MUST be present
#   - @type @author, and @path MUST be defined
#
# @author Kris Yotam
# @type papers
# @path src/app/(content)/papers/content/ai/simulating-moral-judgment-llms.mdx
# ==============================================================================

# ==============================================================================
title: "Simulating Human Moral Judgment in LLMs"
slug: simulating-moral-judgment-llms
date: 2025-07-04
updated: 2026-01-05 16:38:21
status: Notes
certainty: technical-philosophical
importance: 10
author: "Kris Yotam"
description: "Constructs a benchmark from human moral responses to evaluate how closely large language models align with real-world ethical intuitions."
tags: [Ethics, Language Models, Ai Alignment, Benchmarking]
category: ai
sequences: []
cover: 
# ==============================================================================

# Idea
Create a dataset of moral dilemmas (like trolley problems, real-world ethical cases, etc.) and survey how different people 
respond to them. Then use that dataset to evaluate whether existing LLMs (GPT-4, Claude, etc.) mimic human responses, diverge 
in systematic ways, or exhibit biases. Explore implications for alignment.
